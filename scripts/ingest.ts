import fs from 'fs';
import path from 'path';
import { parse } from 'csv-parse/sync';
import crypto from 'crypto';
import * as dotenv from 'dotenv';

// 1. Load environment variables FIRST
dotenv.config({ path: '.env.local' });
dotenv.config(); // Fallback to .env

// Point to the CLEANED dataset generated by the EDA/Cleaning script
const DATA_PATH = path.join(process.cwd(), 'data', 'ted_talks_cleaned.csv');
const CACHE_DIR = path.join(process.cwd(), 'scripts', '.cache');
const MANIFEST_PATH = path.join(CACHE_DIR, 'manifest.json');

// Ensure cache directory exists
if (!fs.existsSync(CACHE_DIR)) {
  fs.mkdirSync(CACHE_DIR, { recursive: true });
}

// Load manifest to track processed chunks (deduplication)
let manifest: Record<string, boolean> = {};
if (fs.existsSync(MANIFEST_PATH)) {
  manifest = JSON.parse(fs.readFileSync(MANIFEST_PATH, 'utf-8'));
}

function saveManifest() {
  fs.writeFileSync(MANIFEST_PATH, JSON.stringify(manifest, null, 2));
}

// Generate a unique hash for text content
function hashText(text: string): string {
  return crypto.createHash('sha256').update(text).digest('hex').substring(0, 16);
}

// Split text into overlapping chunks based on word count
function chunkText(text: string, chunkSize: number, overlapRatio: number): string[] {
  const words = text.split(' ');
  const chunks: string[] = [];
  
  // Calculate how many words to overlap
  const overlapCount = Math.floor(chunkSize * overlapRatio);
  
  let i = 0;
  while (i < words.length) {
    const chunk = words.slice(i, i + chunkSize).join(' ');
    if (chunk.trim().length > 0) chunks.push(chunk);
    
    // Move window forward by (size - overlap)
    i += (chunkSize - overlapCount);
  }
  return chunks;
}

async function main() {
  console.log("ðŸ”§ Loading configuration...");

  // Validate critical API keys before proceeding
  if (!process.env.PINECONE_API_KEY) {
      console.error("âŒ Error: PINECONE_API_KEY is missing via .env.local");
      process.exit(1);
  }
  if (!process.env.LLMOD_API_KEY) {
      console.error("âŒ Error: LLMOD_API_KEY is missing via .env.local");
      process.exit(1);
  }

  // Dynamic Import of libraries (Must happen AFTER env vars are loaded)
  const { RAG_CONFIG } = await import('../src/lib/rag');
  const { openai } = await import('../src/lib/openai');
  const { pinecone } = await import('../src/lib/pinecone');

  // Parse command line arguments
  const args = process.argv.slice(2);
  const dryRun = args.includes('--dry-run');
  const limitArg = args.find(a => a.startsWith('--limit='));
  const limit = limitArg ? parseInt(limitArg.split('=')[1]) : Infinity;

  console.log(`ðŸš€ Starting Ingestion...`);
  console.log(`   Model: ${RAG_CONFIG.embeddingModel}`);
  console.log(`   Config: Chunk=${RAG_CONFIG.chunkSize}, Overlap=${RAG_CONFIG.overlap}`);
  console.log(`   Limit: ${limit === Infinity ? 'ALL' : limit}`);

  // Verify data file exists
  if (!fs.existsSync(DATA_PATH)) {
    console.error(`âŒ Data file not found at ${DATA_PATH}`);
    console.error(`   Did you run 'python scripts/clean_data.py' first?`);
    process.exit(1);
  }

  // Read and Parse CSV
  const fileContent = fs.readFileSync(DATA_PATH, 'utf-8');
  const records = parse(fileContent, {
    columns: true,
    skip_empty_lines: true,
    relax_column_count: true // Helps with minor CSV formatting issues
  });

  console.log(`ðŸ“Š Found ${records.length} talks in cleaned dataset.`);

  // Initialize Pinecone Index
  const indexName = process.env.PINECONE_INDEX_NAME!;
  const index = pinecone.index(indexName);
  
  const vectorsToUpsert: any[] = [];
  let processedCount = 0;
  let newEmbeddingsCount = 0;

  for (const record of records) {
    // Stop if limit reached
    if (processedCount >= limit) break;
    
    const { talk_id, title, speaker_1, published_date, url, transcript } = record;
    
    // Safety check (though cleaned data should be fine)
    if (!transcript) continue;

    // Chunk the transcript
    const chunks = chunkText(transcript, RAG_CONFIG.chunkSize, RAG_CONFIG.overlap);

    for (let i = 0; i < chunks.length; i++) {
      const chunkTextStr = chunks[i];
      const chunkHash = hashText(chunkTextStr);
      const vectorId = `${talk_id}:${i}:${chunkHash}`;

      // Deduplication: Skip if hash already in manifest (unless dry-run)
      if (manifest[chunkHash] && !dryRun) continue; 

      const metadata = {
        talk_id,
        title,
        speaker_1,
        published_date,
        url,
        chunk_index: i,
        text: chunkTextStr
      };

      if (!dryRun) {
        try {
            // Generate Embedding
            const embeddingResponse = await openai.embeddings.create({
              model: RAG_CONFIG.embeddingModel, // Uses RPRTHPB-text-embedding-3-small
              input: chunkTextStr,
              dimensions: RAG_CONFIG.dimensions
            });
            
            vectorsToUpsert.push({
              id: vectorId,
              values: embeddingResponse.data[0].embedding,
              metadata
            });
            
            manifest[chunkHash] = true;
            newEmbeddingsCount++;
        } catch (err: any) {
            console.error(`âŒ Embedding Error on talk ${talk_id}:`, err.message);
        }
      }
    }

    // Upsert to Pinecone in batches of 50
    if (vectorsToUpsert.length >= 50) {
      console.log(`   ðŸ’¾ Upserting batch of ${vectorsToUpsert.length} vectors...`);
      await index.namespace(RAG_CONFIG.namespace).upsert(vectorsToUpsert);
      vectorsToUpsert.length = 0;
      saveManifest();
    }
    
    processedCount++;
    if (processedCount % 50 === 0) console.log(`   ...processed ${processedCount} talks`);
  }

  // Upsert any remaining vectors
  if (vectorsToUpsert.length > 0) {
    console.log(`   ðŸ’¾ Upserting final batch...`);
    await index.namespace(RAG_CONFIG.namespace).upsert(vectorsToUpsert);
    saveManifest();
  }

  console.log(`âœ… Ingestion Complete.`);
  console.log(`   Talks Processed: ${processedCount}`);
  console.log(`   New Embeddings Generated: ${newEmbeddingsCount}`);
}

main().catch(console.error);